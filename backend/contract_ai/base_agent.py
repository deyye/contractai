"""
ä¼˜åŒ–åçš„åŸºç¡€æ™ºèƒ½ä½“ç±»
Optimized Base Agent with Caching, Performance Monitoring and Retry Logic
"""
import json
import logging
import time
import hashlib
from typing import Dict, Any, Optional, List
from functools import lru_cache, wraps
from datetime import datetime, timedelta
from langchain_core.runnables import Runnable
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_deepseek import ChatDeepSeek
from config import Config

# ç®€å•çš„å†…å­˜ç¼“å­˜å®ç°
class SimpleCache:
    """ç®€å•çš„å†…å­˜ç¼“å­˜ç±»"""
    def __init__(self, ttl: int = 3600, max_size: int = 100):
        self.cache: Dict[str, tuple] = {}  # key -> (value, expire_time)
        self.ttl = ttl
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key in self.cache:
            value, expire_time = self.cache[key]
            if datetime.now() < expire_time:
                self.hits += 1
                return value
            else:
                del self.cache[key]
        self.misses += 1
        return None
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        # LRUæ·˜æ±°
        if len(self.cache) >= self.max_size:
            # åˆ é™¤æœ€æ—©è¿‡æœŸçš„é¡¹
            oldest_key = min(self.cache.items(), key=lambda x: x[1][1])[0]
            del self.cache[oldest_key]
        
        expire_time = datetime.now() + timedelta(seconds=self.ttl)
        self.cache[key] = (value, expire_time)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.hits = 0
        self.misses = 0
    
    def stats(self) -> Dict[str, int]:
        """ç¼“å­˜ç»Ÿè®¡"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "size": len(self.cache)
        }

def performance_monitor(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        start_time = time.time()
        try:
            result = func(self, *args, **kwargs)
            elapsed = time.time() - start_time
            
            if Config.LOG_CONFIG.enable_performance_log:
                self.logger.info(
                    f"â±ï¸ {func.__name__} æ‰§è¡Œæ—¶é—´: {elapsed:.2f}ç§’"
                )
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            if not hasattr(self, '_performance_metrics'):
                self._performance_metrics = []
            self._performance_metrics.append({
                "function": func.__name__,
                "elapsed": elapsed,
                "timestamp": datetime.now().isoformat()
            })
            
            return result
        except Exception as e:
            elapsed = time.time() - start_time
            self.logger.error(
                f"âŒ {func.__name__} æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {elapsed:.2f}ç§’): {str(e)}"
            )
            raise
    return wrapper

def retry_on_error(max_retries: int = 3, delay: float = 1.0):
    """é”™è¯¯é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(self, *args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        self.logger.warning(
                            f"âš ï¸ {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}ï¼Œ{delay}ç§’åé‡è¯•"
                        )
                        time.sleep(delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    else:
                        self.logger.error(
                            f"âŒ {func.__name__} æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {str(e)}"
                        )
            raise last_exception
        return wrapper
    return decorator

class BaseAgent(Runnable):
    """ä¼˜åŒ–åçš„åŸºç¡€æ™ºèƒ½ä½“ç±»"""
    
    def __init__(self, agent_name: str, system_prompt: str):
        self.agent_name = agent_name
        self.system_prompt = system_prompt
        self.llm_config = Config.LLM_CONFIG
        self.cache_config = Config.CACHE_CONFIG
        self.performance_config = Config.PERFORMANCE_CONFIG
        self.processing_config = Config.PROCESSING_CONFIG
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(agent_name)
        
        # åˆå§‹åŒ–LLM
        self.llm = ChatDeepSeek(
            api_key=self.llm_config.api_key,
            model=self.llm_config.model,
            temperature=self.llm_config.temperature,
            timeout=self.llm_config.timeout
        )
        
        # åˆå§‹åŒ–ç¼“å­˜
        if self.cache_config.enabled:
            self.cache = SimpleCache(
                ttl=self.cache_config.ttl,
                max_size=self.cache_config.max_size
            )
        else:
            self.cache = None
        
        # æ€§èƒ½æŒ‡æ ‡
        self._performance_metrics = []
        
        self.logger.info(f"âœ… {agent_name} åˆå§‹åŒ–å®Œæˆ (ç¼“å­˜: {self.cache_config.enabled})")
    
    def _generate_cache_key(self, text: str, context: Optional[str] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = text + (context or "")
        return hashlib.md5(content.encode()).hexdigest()
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not self.processing_config.enable_preprocessing:
            return text
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        
        # æˆªæ–­è¿‡é•¿æ–‡æœ¬
        if len(text) > self.processing_config.max_text_length:
            self.logger.warning(
                f"âš ï¸ æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)ï¼Œæˆªæ–­è‡³ {self.processing_config.max_text_length} å­—ç¬¦"
            )
            text = text[:self.processing_config.max_text_length]
        
        return text
    
    def _compress_text(self, text: str) -> str:
        """æ™ºèƒ½æ–‡æœ¬å‹ç¼© - ä¿ç•™å…³é”®ä¿¡æ¯"""
        if not self.processing_config.enable_text_compression:
            return text
        
        if len(text) <= self.processing_config.chunk_size:
            return text
        
        # ç®€å•å‹ç¼©ï¼šä¿ç•™å¼€å¤´å’Œé‡è¦éƒ¨åˆ†
        chunks = []
        chunk_size = self.processing_config.chunk_size
        
        # ä¿ç•™å‰åŠéƒ¨åˆ†
        chunks.append(text[:chunk_size])
        
        # æå–ä¸­é—´é‡è¦å¥å­ï¼ˆåŒ…å«å…³é”®è¯çš„ï¼‰
        important_keywords = ['é£é™©', 'è¿çº¦', 'è´£ä»»', 'ä¹‰åŠ¡', 'æƒåˆ©', 'ä»˜æ¬¾', 'ä»·æ ¼', 'æ ‡å‡†', 'è¦æ±‚']
        middle_text = text[chunk_size:-chunk_size] if len(text) > chunk_size * 2 else ""
        
        if middle_text:
            sentences = middle_text.split('ã€‚')
            important_sentences = [
                s for s in sentences 
                if any(kw in s for kw in important_keywords)
            ][:10]  # æœ€å¤š10å¥
            chunks.extend(important_sentences)
        
        # ä¿ç•™ç»“å°¾éƒ¨åˆ†
        if len(text) > chunk_size:
            chunks.append(text[-chunk_size:])
        
        compressed = 'ã€‚'.join(chunks)
        
        if len(compressed) < len(text):
            self.logger.info(
                f"ğŸ“¦ æ–‡æœ¬å‹ç¼©: {len(text)} -> {len(compressed)} å­—ç¬¦ "
                f"(å‹ç¼©ç‡: {(1 - len(compressed)/len(text)) * 100:.1f}%)"
            )
        
        return compressed
    
    def invoke(self, input: dict, config=None, **kwargs):
        """å®ç°LangChain Runnableæ¥å£"""
        user_text = input.get("text", "")
        context = input.get("context", "")
        return self.process_text_message(user_text, context)
    
    @performance_monitor
    @retry_on_error(max_retries=3, delay=1.0)
    def call_llm(
        self, 
        user_message: str, 
        conversation_history: Optional[List[Dict]] = None,
        use_cache: bool = True
    ) -> str:
        """è°ƒç”¨LLMï¼ˆå¸¦ç¼“å­˜å’Œé‡è¯•æœºåˆ¶ï¼‰"""
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.cache:
            cache_key = self._generate_cache_key(user_message)
            cached_result = self.cache.get(cache_key)
            if cached_result:
                self.logger.info("âœ¨ ç¼“å­˜å‘½ä¸­")
                return cached_result
        
        # é¢„å¤„ç†å’Œå‹ç¼©
        processed_message = self._preprocess_text(user_message)
        if self.processing_config.enable_text_compression:
            processed_message = self._compress_text(processed_message)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [SystemMessage(content=self.system_prompt)]
        
        if conversation_history:
            for msg in conversation_history:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))
        
        messages.append(HumanMessage(content=processed_message))
        
        # è°ƒç”¨LLM
        response = self.llm.invoke(messages)
        result = response.content
        
        # ç¼“å­˜ç»“æœ
        if use_cache and self.cache:
            self.cache.set(cache_key, result)
        
        return result
    
    @performance_monitor
    def process_text_message(self, user_text: str, context: str = "") -> str:
        """å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # éªŒè¯è¾“å…¥
            if len(user_text) < self.processing_config.min_text_length:
                return "è¾“å…¥æ–‡æœ¬è¿‡çŸ­ï¼Œè¯·æä¾›æ›´å¤šå†…å®¹ä»¥è¿›è¡Œåˆ†æã€‚"
            
            self.logger.info(
                f"ğŸ“ å¤„ç†æ¶ˆæ¯: {len(user_text)} å­—ç¬¦"
                + (f" (ä¸Šä¸‹æ–‡: {len(context)} å­—ç¬¦)" if context else "")
            )
            
            # ç»„åˆæ–‡æœ¬å’Œä¸Šä¸‹æ–‡
            full_text = user_text
            if context:
                full_text = f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context}\n\nå¾…åˆ†æå†…å®¹ï¼š\n{user_text}"
            
            # è°ƒç”¨LLM
            response_text = self.call_llm(full_text)
            
            return response_text
            
        except Exception as e:
            self.logger.error(f"âŒ æ¶ˆæ¯å¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
            return f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.cache:
            return self.cache.stats()
        return {"enabled": False}
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self._performance_metrics:
            return {"total_calls": 0}
        
        total_calls = len(self._performance_metrics)
        total_time = sum(m["elapsed"] for m in self._performance_metrics)
        avg_time = total_time / total_calls
        
        return {
            "total_calls": total_calls,
            "total_time": total_time,
            "avg_time": avg_time,
            "recent_calls": self._performance_metrics[-10:]  # æœ€è¿‘10æ¬¡è°ƒç”¨
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache:
            self.cache.clear()
            self.logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")
    
    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.now().isoformat()